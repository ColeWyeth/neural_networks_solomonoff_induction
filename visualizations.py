# %%
import functools
from typing import Any

from absl import app
from absl import logging
import haiku as hk
import jax
import jax.numpy as jnp
import numpy as np
import optax
import tqdm
import tree

import pickle
import matplotlib.pyplot as plt

import sys
nnsi_path = "/root/neural_networks_solomonoff_induction/.."
if f"{nnsi_path}" not in sys.path:
    sys.path.append(f"{nnsi_path}")

from neural_networks_solomonoff_induction.data import data_generator as dg_lib
from neural_networks_solomonoff_induction.data import utm_data_generator as utm_dg_lib
from neural_networks_solomonoff_induction.data import utms as utms_lib
from neural_networks_solomonoff_induction.models import transformer

alphabet_size = 2
probabilistic = True
length = 256

# generate_fresh_batch = False
# visualize = True

rng = np.random.default_rng(seed=8)

# %%
program_sampler = utms_lib.MCSampler(
    rng=rng,
    filename='data/ctx_filtered_binary_probabilistic.pyd', # possibly data/ is necessary
)
# for i in range(10):
#     p = program_sampler.sample_program(length)
#     print(p)

utm = utms_lib.BrainPhoqueUTM(
    program_sampler,
    alphabet_size = alphabet_size,
    print_trace = False,
    shorten_program = True,
    use_input_instruction = probabilistic,
)
data_generator = utm_dg_lib.UTMDataGenerator(
    batch_size=1,
    seq_length=256,
    rng=rng,
    utm=utm,
    memory_size=200, # matches optimize_Q
    maximum_steps=1024, # matches optimize_Q
    tokenizer=utm_dg_lib.Tokenizer.SEQ_POSITION,
    maximum_program_length=256,
)

batch, log_dict = data_generator.sample()
batch = np.array(batch)

print(batch.shape)
binary_batch = batch.argmax(axis=-1)
print(binary_batch.shape)
#print(f"{binary_batch=}")
for row in binary_batch:
    print(row)
print(log_dict)

# with open('binary_batch_log_dict.pkl', 'wb') as f:
#     pickle.dump((binary_batch,log_dict),f)

# with open('binary_batch_log_dict.pkl', 'rb') as f:
#     binary_batch, log_dict = pickle.load(f)

# %%
config = transformer.TransformerConfig(vocab_size=alphabet_size)
model = hk.transform(
    functools.partial(transformer.transformer_decoder, config=config)
)
fname = "params_mid_train.npz"
with open(fname, "rb") as f:
    loaded_params = np.load(f, allow_pickle=True)
    params = dict(loaded_params)
    for k in params.keys():
        params[k] = params[k].item()
conditionals = model.apply(
    params=params,
    targets=binary_batch,
    rng=None,
)
print(f"{conditionals.shape=}")
true_conditionals = jnp.take_along_axis(
    conditionals, binary_batch[..., None], axis=-1
)[..., 0]
true_conditionals = jnp.where(log_dict['loss_mask'], 0.0, true_conditionals)
print(true_conditionals)
cond_probs = jax.numpy.exp(true_conditionals) # don't take seriously, check mask

avg_log_loss = -true_conditionals.mean(axis=0)
avg_cond_prob = cond_probs.mean(0)
print(f"{avg_log_loss=}")

print(f"{avg_cond_prob=}")

total_loss = -true_conditionals.sum(axis=1)
print(f"{total_loss=}")
print([res['short_ln_loss'] for res in log_dict['results']])
plt.plot(avg_log_loss)
# plt.plot(avg_cond_prob)

# %%
# Prompt design 

# string samples
prompt = "The following binary sequence was generated by a simple program, represented as comma-separated bits of output: "

str_batch = [
    prompt + ','.join([str(digit) for digit in sample]) for sample in binary_batch 
]
print(str_batch[0])
print(len(str_batch))



# %%
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

#torch.cuda.empty_cache()

# Load GPT-2 and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)
model.eval()

# Set device to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Input text
text = "The quick brown fox jumps over the lazy dog."

# Tokenize input
inputs = tokenizer(str_batch, return_tensors="pt")
print(inputs)
input_ids = inputs["input_ids"].to(device)

# Run model
with torch.no_grad():
    outputs = model(input_ids, labels=input_ids)
    logits = outputs.logits

# Shift logits and labels to compute log probabilities
log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

# Gather log probabilities of the correct tokens
# [batch_size, seq_len, vocab_size] -> [batch_size, seq_len]
token_log_probs = log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)

# Decode tokens and print with log probs
tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
for token, log_prob in zip(tokens, token_log_probs[0]):
    print(f"{token:>12} : {log_prob.item():.4f}")




# still need to handle masking, see _make_loss_fn
# %%
# Evaluate LLMs
from anthropic import Anthropic
from openai import OpenAI

from pprint import pprint

assert os.getenv("OPENAI_API_KEY") is not None, "You must set your OpenAI API key - see instructions in dropdown"
assert os.getenv("ANTHROPIC_API_KEY") is not None, "You must set your Anthropic API key - see instructions in dropdown"

# OPENAI_API_KEY

openai_client = OpenAI()
anthropic_client = Anthropic()

# %%
response = openai_client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"},
    ],
    n=2,
    logprobs=True,
)

pprint(response.model_dump())  # See the entire ChatCompletion object, as a dict (more readable)
print("\n", response.choices[0].message.content)  # See the response message only
# %%
